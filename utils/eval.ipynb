{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "\n",
    "baseline = '../data/generated_conversations/bm25_t5_monot5_bart/bm25_t5_monot5_bart.run'\n",
    "prompted_llama = \"../data/generated_conversations/prompted_simulation_baseline_v2/prompted_simulation_baseline_v2.run\"\n",
    "tuned_llama = \"../data/generated_conversations/bm25_t5_bart_llama2_t5_bart/bm25_t5_bart_llama2_t5_bart.run\"\n",
    "prompted_openai = \"../data/generated_conversations/gpt3_simulation_baseline/gpt3_simulation_baseline.run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRR = ir_measures.RR()\n",
    "RECALL = ir_measures.R()@1000\n",
    "NDCG_CUT_3 = ir_measures.nDCG@3\n",
    "NDCG_CUT_1000 = ir_measures.nDCG@1000\n",
    "MAP = ir_measures.AP()@1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20-2_2', '20-1_2', '18-2_4', '18-2_3', '18-1_15', '18-1_13', '18-1_10', '17-3_9', '17-3_14', '17-3_1', '17-2_10', '17-1_9', '17-1_1', '16-2_2', '16-1_7', '16-1_6', '15-2_3', '14-2_8', '14-1_2', '14-1_17', '11-2_2', '11-1_1', '10-3_4', '10-3_2', '10-2_7', '10-2_6', '10-2_3']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "turns_to_score = []\n",
    "\n",
    "with open(\"annotations.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for idx, line in enumerate(reader):\n",
    "        if idx < 2:\n",
    "            continue\n",
    "        turn_id, prompted_llama_annotation, prompted_openai_annotation, tuned_llama_annotation = line\n",
    "        # [\"follow_up\", \"positive_feedback\", \"good_cq_answer\"] -> follow_up\n",
    "        # [\"unrelated_feedback\"] -> unrelated_feedback\n",
    "        # [\"negative_with_clarification\"] -> negative_with_clarification\n",
    "        # [\"negative_feedback\"] -> negative_feedback\n",
    "        # [\"abdication\"] -> abdication\n",
    "        if tuned_llama_annotation in [\"abdication\"]:\n",
    "            turns_to_score.append(turn_id)\n",
    "print(turns_to_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = list(ir_measures.read_trec_qrels('../../data/datasets/ikat/2023-qrels.all-turns.txt'))\n",
    "qrels = [q for q in qrels if q.query_id in turns_to_score]\n",
    "run = ir_measures.read_trec_run(tuned_llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{nDCG@1000: 0.13833351052250117,\n",
       " R@1000: 0.18101876121678104,\n",
       " nDCG@3: 0.1346389360644392,\n",
       " AP@1000: 0.05581918952150833,\n",
       " RR: 0.33650755374893304}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_measures.calc_aggregate([MRR, RECALL, NDCG_CUT_3, NDCG_CUT_1000, MAP], qrels, run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
