{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ir_measures\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "# Load the base data.\n",
    "with open('../../data/datasets/cast/year_4/annotated_topics.json', 'r') as f:\n",
    "    annotated_topics = json.load(f)\n",
    "# Load the qrels.\n",
    "qrels = list(ir_measures.read_trec_qrels(\n",
    "    '../../data/datasets/cast/year_4/cast2022.qrel'))\n",
    "# load the searcher.\n",
    "searcher = LuceneSearcher('../../data/indexes/sparse/cast/trecweb_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:10<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parse original data.\n",
    "def get_text(doc_id: str):\n",
    "    \"\"\"Returns the passage text for a given doc_id.\"\"\"\n",
    "    doc_id, passage_id = doc_id.rsplit(\"-\", 1)\n",
    "    document = searcher.doc(doc_id).raw()\n",
    "    document = bs(document, \"lxml\")\n",
    "    passages = document.find_all(\"passage\")\n",
    "    for idx, passage in enumerate(passages):\n",
    "        if idx == int(passage_id):\n",
    "            return passage.text\n",
    "        \n",
    "stash = dict()\n",
    "parsed_turns = set()\n",
    "for topic in tqdm.tqdm(annotated_topics):\n",
    "    for index, turn in enumerate(topic['turn']):\n",
    "        turn_id = f\"{topic['number']}_{turn['number']}\"\n",
    "        if turn_id in parsed_turns:\n",
    "            continue\n",
    "        information_need = turn.get(\"information_need\")\n",
    "        utterances = []\n",
    "\n",
    "        for previous_turn in topic['turn'][:index]:\n",
    "            utterances.append({\n",
    "                'User': previous_turn.get(\"utterance\"),\n",
    "                'System': previous_turn.get(\"response\")\n",
    "            })\n",
    "        utterances.append({'User': turn.get(\"utterance\")})\n",
    "        \n",
    "        passage_ids_with_relevance = [\n",
    "            {'passage_id': qrel.doc_id, 'relevance': qrel.relevance} for qrel in qrels if qrel.query_id == turn_id]\n",
    "        passage_texts = [\n",
    "            get_text(item['passage_id']) for item in passage_ids_with_relevance]\n",
    "        \n",
    "        stash[turn_id] = {\n",
    "            'information_need': information_need,\n",
    "            'passages': passage_texts,\n",
    "            'passage_ids_with_relevance': passage_ids_with_relevance\n",
    "        }\n",
    "        \n",
    "        dataset.append({\n",
    "            'information_need': information_need,\n",
    "            'utterances': utterances,\n",
    "            'passages': passage_texts,\n",
    "            'passage_ids_with_relevance': passage_ids_with_relevance\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import re\n",
    "import hashlib\n",
    "# Parse generated data.\n",
    "def load_cast_convsim_conversations(\n",
    "        path: str = '../../data/transcripts/convsim_outputs'\n",
    "    ):\n",
    "    \"\"\"Loads the conversations for training.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the conversations shelve db.\n",
    "    \"\"\"\n",
    "    seen_conversations = set()\n",
    "\n",
    "    for directory in tqdm.tqdm(pathlib.Path(path).iterdir()):\n",
    "        for subdirectory in directory.iterdir():\n",
    "            if not subdirectory.is_dir():\n",
    "                continue\n",
    "            for file in subdirectory.iterdir():\n",
    "                # Get the file basename\n",
    "                basename = file.name.split('.')[0]\n",
    "                match = re.match(r'^\\d+_\\d+-\\d+$', basename)\n",
    "                if match:\n",
    "                    basename = match.string\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                with open(file, 'r') as f:\n",
    "                    try:\n",
    "                        conversation = json.load(f)\n",
    "                    except json.decoder.JSONDecodeError:\n",
    "                        continue\n",
    "                # Count feedback turns.\n",
    "                feedback_turns = [\n",
    "                    turn for turn in conversation if turn['type'] == 'feedback']\n",
    "                long_feedback_turns = False\n",
    "                # Check the content of the feedback turns\n",
    "                for turn in feedback_turns:\n",
    "                    if len(turn['utterance'].split()) > 25:\n",
    "                        long_feedback_turns = True\n",
    "                    \n",
    "                if len(feedback_turns) > 2 or len(feedback_turns) == 0 or long_feedback_turns:\n",
    "                    continue\n",
    "\n",
    "                # Get the hash of the conversation\n",
    "                conversation_hash = hashlib.sha256(\n",
    "                    json.dumps(conversation).encode('utf-8')).hexdigest()\n",
    "                \n",
    "                if conversation_hash in seen_conversations:\n",
    "                    continue\n",
    "                seen_conversations.add(conversation_hash)\n",
    "\n",
    "                # Parse conversations.\n",
    "                parsed_conversation = []\n",
    "                current_turn = {}\n",
    "                for turn in conversation:\n",
    "                    if turn['participant'] == \"User\":\n",
    "                        current_turn['User'] = turn['utterance']\n",
    "                    else:\n",
    "                        current_turn['System'] = turn['utterance']\n",
    "                        parsed_conversation.append(current_turn)\n",
    "                        current_turn = {}\n",
    "\n",
    "                dataset.append({\n",
    "                    'information_need': stash[basename]['information_need'],\n",
    "                    'utterances': parsed_conversation,\n",
    "                    'passages': stash[basename]['passages'],\n",
    "                    'passage_ids_with_relevance': stash[basename]['passage_ids_with_relevance']\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172it [01:42,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "load_cast_convsim_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2699924\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in dataset:\n",
    "    count += len(item['passages'])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
